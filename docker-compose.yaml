services:
  source_postgres:
    image: postgres:latest
    ports:
      - "5435:5432"   # STREET→APARTMENT: use localhost:5435 from your laptop; inside container Postgres listens on 5432
    networks: [elt_network]   # Same hallway so containers can talk by name: "source_postgres"
    environment:
      POSTGRES_DB: source_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    volumes:
      # Day-one seed only (runs on first start of a fresh container/volume)
      - ./source_db_init/init.sql:/docker-entrypoint-initdb.d/source_db_init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d source_db"]
      interval: 5s
      timeout: 5s
      retries: 20

  destination_postgres:
    image: postgres:latest
    ports:
      - "5434:5432"   # STREET→APARTMENT: use localhost:5434 from laptop; inside, Postgres listens on 5432
    networks: [elt_network]
    environment:
      POSTGRES_DB: destination_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: secret
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d destination_db"]
      interval: 5s
      timeout: 5s
      retries: 20
    # TIP for persistence of this *business* DB (optional):
    # volumes:
    #   - dest_data:/var/lib/postgresql/data

  # Airflow METADATA database (persists DAG runs, tasks, Variables, Connections, etc.)
  postgres:
    image: postgres:17
    ports:
      - "5436:5432"
    networks: [elt_network]
    volumes:
      - airflow_metadata:/var/lib/postgresql/data   # <-- Named volume so metadata survives restarts
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 5s
      retries: 20

  # One-time DB migrations for Airflow
  init-airflow:
    image: apache/airflow:3.0.6-python3.12
    depends_on:
      postgres:
        condition: service_healthy
    networks: [elt_network]
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      # Install Docker provider + SDK before anything else so parsing works first time
      PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-docker docker"
    command: bash -lc "airflow db migrate"
    restart: "no"

  webserver:
    image: apache/airflow:3.0.6-python3.12
    user: root    # to access /var/run/docker.sock; drop to 'airflow' in hardened setups
    depends_on:
      postgres:
        condition: service_healthy
      init-airflow:
        condition: service_completed_successfully
    networks: [elt_network]
    extra_hosts:
      - "host.docker.internal:host-gateway"   # only needed if tasks call back to the host
    volumes:
      - ./custom_postgres/airflow/DAGs:/opt/airflow/dags        # DAG code → Airflow’s DAGs folder
      - ./elt:/opt/airflow/dags/elt                             # helper scripts used by your PythonOperator
      - /var/run/docker.sock:/var/run/docker.sock               # lets DockerOperator talk to Docker daemon
      - ./airflow_logs:/opt/airflow/logs
      # (Removed /dbt mounts here; they aren’t used by Airflow itself—your DockerOperator mounts dbt separately)
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: gQgtwrkWj32D6vaQ5aTrnnGNdIx3wSqNxzHw5TfzQLk=

      # API auth secret shared by webserver & scheduler
      AIRFLOW__API_AUTH__JWT_SECRET: -e0eRDU75K1gAgQ706OWi1V7ez5cg-ZYHy6QgaKBMOAn4IrHdrArjLzowKbkRekXcZb8Tbs4-WT6_pYph0qp8Q
      AIRFLOW__API__SECRET_KEY: secreb_7ik-D9OApBW1O7RZp_JDz25Van4HvyCwDpr_UH43k7P761za2UucXBY7CcYqmtFN9ShR_O1Wrs-MsYefvC_gt
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth

      # << KEY FIXES >>
      # Where the Task SDK calls back (base host:port)
      AIRFLOW__SDK__API_URL: http://webserver:8080
      # Where the Task Execution API lives (note the /execution/ suffix)
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://webserver:8080/execution/

      # Logs folder you’ve mounted
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs

      # Keep your extra deps
      PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-docker docker"

    healthcheck:
      test: ["CMD-SHELL", "bash -lc 'curl -sf http://localhost:8080/api/v2/monitor/health || exit 1'"]
      interval: 5s
      timeout: 5s
      retries: 30
    ports:
      - "8080:8080"
    command: ["airflow", "api-server", "--host", "0.0.0.0", "--port", "8080"]
    restart: unless-stopped

  scheduler:
    image: apache/airflow:3.0.6-python3.12
    user: root
    depends_on:
      postgres:
        condition: service_healthy
      webserver:
        condition: service_started
      init-airflow:
        condition: service_completed_successfully
    networks: [elt_network]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ./custom_postgres/airflow/DAGs:/opt/airflow/dags
      - ./elt:/opt/airflow/dags/elt
      - /var/run/docker.sock:/var/run/docker.sock
      - ./airflow_logs:/opt/airflow/logs
    
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: gQgtwrkWj32D6vaQ5aTrnnGNdIx3wSqNxzHw5TfzQLk=

      # API auth secret shared by webserver & scheduler
      AIRFLOW__API_AUTH__JWT_SECRET: -e0eRDU75K1gAgQ706OWi1V7ez5cg-ZYHy6QgaKBMOAn4IrHdrArjLzowKbkRekXcZb8Tbs4-WT6_pYph0qp8Q
      AIRFLOW__API__SECRET_KEY: secreb_7ik-D9OApBW1O7RZp_JDz25Van4HvyCwDpr_UH43k7P761za2UucXBY7CcYqmtFN9ShR_O1Wrs-MsYefvC_gt
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth

      # << KEY FIXES >>
      # Where the Task SDK calls back (base host:port)
      AIRFLOW__SDK__API_URL: http://webserver:8080
      # Where the Task Execution API lives (note the /execution/ suffix)
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://webserver:8080/execution/

      # Logs folder you’ve mounted
      AIRFLOW__LOGGING__BASE_LOG_FOLDER: /opt/airflow/logs

      # Keep your extra deps
      PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-docker docker"


    command: ["airflow", "scheduler"]
    restart: unless-stopped

  dbt_image:   # build-only target for your custom dbt image
    build:
      context: ./dbt          # folder that has your Dockerfile + dbt project
      dockerfile: Dockerfile  # adjust if your file isn’t literally called Dockerfile
    image: my-dbt:1.7-pg      # tag that matches what your DAG is using
    networks:
      - elt_network
    # This service doesn’t need volumes/ports/commands because Airflow runs dbt via DockerOperator

# ---- Named volumes (Docker-managed storage) ----
volumes:
  airflow_metadata: {}   # persists Airflow metadata DB files under Docker’s volumes dir
  # dest_data: {}        # uncomment if you enable destination_postgres persistence
  # src_data: {}         # uncomment if you enable source_postgres persistence

networks:
  elt_network:
    name: elt_network
    driver: bridge
